{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hx3U35iEAKi2",
        "outputId": "e93b6dfe-1eea-43a1-8550-ffa2df00094c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mask-CycleGAN'...\n",
            "remote: Enumerating objects: 16165, done.\u001b[K\n",
            "remote: Counting objects: 100% (216/216), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 16165 (delta 122), reused 135 (delta 75), pack-reused 15949\u001b[K\n",
            "Receiving objects: 100% (16165/16165), 590.23 MiB | 30.14 MiB/s, done.\n",
            "Resolving deltas: 100% (126/126), done.\n",
            "Updating files: 100% (16182/16182), done.\n",
            "/content/Mask-CycleGAN\n",
            "Mounted at /content/drive\n",
            "Loading model: \n",
            "Loading model: \n",
            "Loading model: \n",
            "Loading model: \n",
            "Loading model: \n",
            "Loading model: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 266/266 [10:21<00:00,  2.34s/it]\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Federico6419/Mask-CycleGAN          #It clones our github repository\n",
        "%cd Mask-CycleGAN\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#IMPORTS\n",
        "from dataset import Dataset\n",
        "import config\n",
        "from discriminator import Discriminator\n",
        "from generator import Generator\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "from torch import Tensor\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import random\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import math\n",
        "\n",
        "\n",
        "######### FUNCTIONS FOR SAVE AND LOAD MODELS #########\n",
        "#This function saves the weights of the model in a file\n",
        "def save_model(model, optimizer, epoch, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"Saving model for epoch : \"+ str(epoch))\n",
        "\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }, filename)\n",
        "\n",
        "\n",
        "#This function loads the precomputed weights of the model from a file\n",
        "def load_model(file, model, optimizer, lr):\n",
        "    print(\"Loading model: \")\n",
        "    model_check = torch.load(file, map_location=config.DEVICE)\n",
        "    model.load_state_dict(model_check[\"state_dict\"])\n",
        "    optimizer.load_state_dict(model_check[\"optimizer\"])\n",
        "\n",
        "    #epoch =model_check[\"epoch\"]\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "######### END FUNCTIONS FOR SAVE AND LOAD MODELS ##########\n",
        "\n",
        "\n",
        "######### FUNCTIONS THAT GENERATES A RANDOM MASKS ##########\n",
        "def mask_generator(mask_type):\n",
        "    #If the type of mask is sqared\n",
        "    if(mask_type == \"Squared\"):\n",
        "        scales = [0.5, 0.8, 1.0]\n",
        "\n",
        "        current_scale = random.choice(scales)\n",
        "\n",
        "        imageSize = 256                   #Size of the image\n",
        "        image_center = imageSize // 2\n",
        "\n",
        "        i = int(image_center - image_center * current_scale)\n",
        "        j = int(image_center + image_center * current_scale)\n",
        "\n",
        "        mask = torch.zeros(3, imageSize, imageSize, requires_grad=False)\n",
        "\n",
        "        mask[:, i:j, i:j] = 1.0\n",
        "        return mask\n",
        "\n",
        "    #If the type of mask is rectangular\n",
        "    elif(mask_type == \"Rectangular\"):\n",
        "        #With a probability of 20% we have a full mask\n",
        "        if random.random() < 0.2:\n",
        "            imageSize = 256                   #Size of the image\n",
        "            mask = torch.zeros(3, imageSize, imageSize, requires_grad=False)\n",
        "            mask[:, :, :] = 1.0\n",
        "            return mask\n",
        "\n",
        "        else:\n",
        "            maxNum = 5                        #Maximum number of rectangles to draw\n",
        "            minNum = int(random.uniform(1, 5))       #Minimum number of rectangles to draw\n",
        "\n",
        "            minArea = 0.15                    #Minimal accumulative area relative to the whole image area\n",
        "\n",
        "            imageSize = 256                   #Size of the image\n",
        "            minRectSize = imageSize/10        #Minimum size of the rectangles\n",
        "            maxRectSize= imageSize            #Maximum size of the rectangles\n",
        "\n",
        "            numRects = 0                      #Initialize the number of rectangles to 0\n",
        "            sumRelArea = 0.0\n",
        "            mask = torch.zeros(3, imageSize, imageSize, requires_grad=False)\n",
        "\n",
        "            while((numRects < minNum) or (sumRelArea < minArea)):\n",
        "                #Randomly generate the top left corner of the rectangle\n",
        "                i0 = int(random.uniform(0, imageSize - minRectSize))\n",
        "                j0 = int(random.uniform(0, imageSize - minRectSize))\n",
        "\n",
        "                #Randomly generate the bottom right corner of the rectangle.\n",
        "                i1 = int(random.uniform(i0 + minRectSize, min(i0 + maxRectSize, imageSize)))\n",
        "                j1 = int(random.uniform(j0 + minRectSize, min(j0 + maxRectSize, imageSize)))\n",
        "\n",
        "                #Draw rectangle on the mask\n",
        "                mask[:, i0:i1, j0:j1] = 1.0\n",
        "                numRects += 1\n",
        "                sumRelArea += ((i1 - i0) * (j1 - j0)) / (imageSize * imageSize)\n",
        "\n",
        "            return mask\n",
        "\n",
        "\n",
        "#Circular mask generator\n",
        "def circular_mask_generator(scale):\n",
        "\n",
        "    imageSize = 256                   #Size of the image\n",
        "    mask = torch.zeros(3, imageSize, imageSize, requires_grad=False)\n",
        "\n",
        "    center = (int(256/2), int(256/2))\n",
        "\n",
        "    radius = (256 // 2) * scale\n",
        "\n",
        "    for x in range(256):\n",
        "        for y in range(256):\n",
        "            dist_from_center = math.sqrt((x - center[0])**2 + (y-center[1])**2)\n",
        "            if(dist_from_center <= radius):\n",
        "                mask[:, x, y]=1.0\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################### TRAIN FUNCTION #########################\n",
        "def train_fn(disc_A, disc_B, disc_AM, disc_BM, gen_B, gen_A, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler,LAMBDA_IDENTITY, LAMBDA_CYCLE, LAMBDA_MASK, LAMBDA_CYCLE_MASK, TRAIN_MASK):\n",
        "\n",
        "    loop = tqdm(loader, leave=True)           #leave=True to avoid print newline\n",
        "\n",
        "    for idx, (domainB, domainA) in enumerate(loop):                             #It loops over the images from domain A and domain B\n",
        "        domainA = domainA.to(config.DEVICE)                                     #Its puts the images from the two domains one the device\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "\n",
        "        #Label printed every epoch to see the prediction of the discriminators\n",
        "        A_is_real = 0\n",
        "        A_is_fake = 0\n",
        "        AM_is_real = 0\n",
        "        AM_is_fake = 0\n",
        "        B_is_real = 0\n",
        "        B_is_fake = 0\n",
        "        BM_is_real = 0\n",
        "        BM_is_fake = 0\n",
        "\n",
        "\n",
        "        #Create a random Mask\n",
        "        mask = mask_generator(TRAIN_MASK)\n",
        "        mask = mask.to(config.DEVICE)\n",
        "\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            ############## TRAIN DISCRIMINATOR DOMAIN B #############\n",
        "            fake_B = gen_B(domainA, mask)              #Generate with Generator a fake image from domain B starting from an image from domain A\n",
        "\n",
        "            #Compute probability of the real image and of the fake image to be a real image from domain B using the Discriminator\n",
        "            D_B_real = disc_B(domainB)\n",
        "            D_B_fake = disc_B(fake_B.detach())\n",
        "\n",
        "            #Used to print the percentage that the given image is predicted real or fake !!!!\n",
        "            B_is_real += D_B_real.mean().item()\n",
        "            B_is_fake += D_B_fake.mean().item()\n",
        "\n",
        "            #Compute the Mean Squared Error\n",
        "            D_B_real_loss = mse(D_B_real, torch.ones_like(D_B_real))    #MSE computed between the prediction of the real image made by Discriminator and a Tensor composed by all ones\n",
        "            D_B_fake_loss = mse(D_B_fake, torch.zeros_like(D_B_fake))   #MSE computed between the prediction of the fake image made by Discriminator and a Tensor composed by all zeros\n",
        "            D_B_loss = D_B_real_loss + D_B_fake_loss                    #Sum the real image loss and the fake image loss\n",
        "\n",
        "\n",
        "\n",
        "            ############## TRAIN MASK DISCRIMINATOR DOMAIN B #############\n",
        "            #fake_B = gen_B(domainA)              #Generate with Generator a fake image from domain B starting from an image from domain A\n",
        "\n",
        "            #Compute probability of the real image and of the fake image to be a real image from domain B using the Discriminator\n",
        "            DM_B_real = disc_BM(domainB*mask)\n",
        "            DM_B_fake = disc_BM((fake_B*mask).detach())\n",
        "\n",
        "            #Used to print the percentage that the given image is predicted real or fake !!!!\n",
        "            BM_is_real += DM_B_real.mean().item()\n",
        "            BM_is_fake += DM_B_fake.mean().item()\n",
        "\n",
        "            #Compute the Mean Squared Error\n",
        "            DM_B_real_loss = mse(DM_B_real, torch.ones_like(DM_B_real))    #MSE computed between the prediction of the real image made by Discriminator and a Tensor composed by all ones\n",
        "            DM_B_fake_loss = mse(DM_B_fake, torch.zeros_like(DM_B_fake))   #MSE computed between the prediction of the fake image made by Discriminator and a Tensor composed by all zeros\n",
        "            DM_B_loss = DM_B_real_loss + DM_B_fake_loss                    #Sum the real image loss and the fake image loss\n",
        "\n",
        "\n",
        "            ########### SUM TWO DISCRIMINATORS FROM DOMAIN B ##############\n",
        "            D_B_total_loss = LAMBDA_MASK * DM_B_loss + (1 - LAMBDA_MASK) * D_B_loss\n",
        "\n",
        "\n",
        "\n",
        "            ########### TRAIN DISCRIMINATOR OF THE DOMAIN A ##############\n",
        "            fake_A = gen_A(domainB, mask)             #Generate with Generator a fake image from domain A starting from an image from domain B\n",
        "\n",
        "            #Compute probability of the real image and of the fake image to be a real image from domain B using the Discriminator\n",
        "            D_A_real = disc_A(domainA)\n",
        "            D_A_fake = disc_A(fake_A.detach())\n",
        "\n",
        "            #Used print the percentage that the given image is predicted real or fake !!!!\n",
        "            A_is_real += D_A_real.mean().item()\n",
        "            A_is_fake += D_A_fake.mean().item()\n",
        "\n",
        "            #Compute the Mean Squared Error\n",
        "            D_A_real_loss = mse(D_A_real, torch.ones_like(D_A_real))    #MSE computed between the prediction of the real image made by Discriminator and a Tensor composed by all ones\n",
        "            D_A_fake_loss = mse(D_A_fake, torch.zeros_like(D_A_fake))   #MSE computed between the prediction of the fake image made by Discriminator and a Tensor composed by all zeros\n",
        "            D_A_loss = D_A_real_loss + D_A_fake_loss                    #Sum the real image loss and the fake image loss\n",
        "\n",
        "\n",
        "            ############## TRAIN MASK DISCRIMINATOR DOMAIN A #############\n",
        "            #fake_A = gen_A(domainB)              #Generate with Generator a fake image from domain B starting from an image from domain A\n",
        "\n",
        "            #Compute probability of the real image and of the fake image to be a real image from domain B using the Discriminator\n",
        "            DM_A_real = disc_AM(domainA*mask)\n",
        "            DM_A_fake = disc_AM((fake_A*mask).detach())\n",
        "\n",
        "            #Used to print the percentage that the given image is predicted real or fake !!!!\n",
        "            AM_is_real += DM_A_real.mean().item()\n",
        "            AM_is_fake += DM_A_fake.mean().item()\n",
        "\n",
        "            #Compute the Mean Squared Error\n",
        "            DM_A_real_loss = mse(DM_A_real, torch.ones_like(DM_A_real))    #MSE computed between the prediction of the real image made by Discriminator and a Tensor composed by all ones\n",
        "            DM_A_fake_loss = mse(DM_A_fake, torch.zeros_like(DM_A_fake))   #MSE computed between the prediction of the fake image made by Discriminator and a Tensor composed by all zeros\n",
        "            DM_A_loss = DM_A_real_loss + DM_A_fake_loss                    #Sum the real image loss and the fake image loss\n",
        "\n",
        "\n",
        "            ########### SUM TWO DISCRIMINATORS FROM DOMAIN A ##############\n",
        "            D_A_total_loss = LAMBDA_MASK * DM_A_loss + (1 - LAMBDA_MASK) * D_A_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #Put together the loss of the two discriminators\n",
        "            D_loss = (D_A_total_loss + D_B_total_loss)/2\n",
        "\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        d_scaler.scale(D_loss).backward(retain_graph=True)\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "\n",
        "\n",
        "        ########################## TRAIN GENERATORS #########################\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            #Compute the Discriminator predictions\n",
        "            D_A_fake = disc_A(fake_A)\n",
        "            D_B_fake = disc_B(fake_B)\n",
        "            D_A_real = disc_A(domainA)\n",
        "            D_B_real = disc_B(domainB)\n",
        "\n",
        "            #Compute the GAN losses\n",
        "            loss_G_A = 0\n",
        "            loss_G_B = 0\n",
        "\n",
        "            loss_G_A = mse(D_A_fake, torch.ones_like(D_A_fake))  #Compute the MSE between the prediction of the Discriminator of the fake image from domain A and a tensor with all ones\n",
        "            loss_G_B = mse(D_B_fake, torch.ones_like(D_B_fake))  #Compute the MSE between the prediction of the Discriminator of the fake image from domain B and a tensor with all ones\n",
        "\n",
        "\n",
        "            #Compute the Mask Discriminator predictions\n",
        "            DM_A_fake = disc_AM(fake_A * mask)\n",
        "            DM_B_fake = disc_BM(fake_B * mask)\n",
        "            DM_A_real = disc_AM(domainA * mask)\n",
        "            DM_B_real = disc_BM(domainB * mask)\n",
        "\n",
        "            #Compute the GAN losses\n",
        "            loss_GM_A = 0\n",
        "            loss_GM_B = 0\n",
        "\n",
        "            loss_GM_A = mse(DM_A_fake, torch.ones_like(DM_A_fake))  #Compute the MSE between the prediction of the Discriminator of the fake image from domain A and a tensor with all ones\n",
        "            loss_GM_B = mse(DM_B_fake, torch.ones_like(DM_B_fake))  #Compute the MSE between the prediction of the Discriminator of the fake image from domain B and a tensor with all ones\n",
        "\n",
        "\n",
        "            #CYCLE LOSS\n",
        "            cycle_A = gen_A(fake_B, mask)\n",
        "            cycle_B = gen_B(fake_A, mask)\n",
        "            cycle_A_loss = l1(domainA, cycle_A)\n",
        "            cycle_B_loss = l1(domainB, cycle_B)\n",
        "\n",
        "\n",
        "            #CYCLE MASK LOSS\n",
        "            fake_AM = gen_A(domainB, (1 - mask))\n",
        "            fake_BM = gen_B(domainA, (1 -mask))\n",
        "            cycle_A = gen_A(fake_BM, (1 - mask))\n",
        "            cycle_B = gen_B(fake_AM, (1 - mask))\n",
        "            cycle_AM_loss = l1(domainA * (1 - mask), cycle_A)\n",
        "            cycle_BM_loss = l1(domainB * (1 - mask), cycle_B)\n",
        "\n",
        "            #SUM THE CYCLE LOSSES\n",
        "            cycle_total_loss_A = LAMBDA_CYCLE_MASK * cycle_A_loss + (1 - LAMBDA_CYCLE_MASK) * cycle_AM_loss\n",
        "            cycle_total_loss_B = LAMBDA_CYCLE_MASK * cycle_B_loss + (1 - LAMBDA_CYCLE_MASK) * cycle_BM_loss\n",
        "\n",
        "\n",
        "            #IDENTITY LOSS\n",
        "            identity_A = gen_A(domainA, mask)\n",
        "            identity_B = gen_B(domainB, mask)\n",
        "            identity_loss_A = l1(domainA, identity_A)\n",
        "            identity_loss_B = l1(domainB, identity_B)\n",
        "\n",
        "            #Add all losses together, multiplied by their relative parameter\n",
        "            G_loss = (\n",
        "                loss_G_B * (1 - LAMBDA_MASK)\n",
        "                + loss_G_A * (1 - LAMBDA_MASK)\n",
        "                + loss_GM_B * LAMBDA_MASK\n",
        "                + loss_GM_A * LAMBDA_MASK\n",
        "                + cycle_total_loss_B * LAMBDA_CYCLE\n",
        "                + cycle_total_loss_A * LAMBDA_CYCLE\n",
        "                + identity_loss_A * LAMBDA_IDENTITY\n",
        "                + identity_loss_B * LAMBDA_IDENTITY\n",
        "            )\n",
        "\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward(retain_graph=True)\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        ##########################  END TRAIN GENERATORS #########################\n",
        "\n",
        "\n",
        "        #Save tensors into images every 150 to see in real time the progress of the net\n",
        "        if idx % 150 == 0:\n",
        "            save_image(mask, f\"Saved_Images/Mask_{idx}.png\")\n",
        "            save_image(fake_B*0.5+0.5, f\"Saved_Images/domainB_{idx}.png\")\n",
        "            save_image(fake_A*0.5+0.5, f\"Saved_Images/domainA_{idx}.png\")\n",
        "\n",
        "        #Set postfixes to the progess bar of tqdm\n",
        "        #loop.set_postfix(A_real=A_is_real/(idx+1), A_fake=A_is_fake/(idx+1),B_real=B_is_real/(idx+1), B_fake=B_is_fake/(idx+1))\n",
        "        loop.set_postfix(G_loss=G_loss.item(), D_loss=D_loss.item(), cycle_A_loss=cycle_A_loss.item(), cycle_B_loss=cycle_B_loss.item())\n",
        "\n",
        "########################### END TRAIN FUNCTION ######################\n",
        "\n",
        "\n",
        "\n",
        "#TEST FUNCTION\n",
        "def test_fn(gen_B, gen_A, test_loader, mask_type):\n",
        "\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "\n",
        "    \"\"\"\n",
        "    for idx, (domainB, domainA) in enumerate(loop):\n",
        "        if(mask_type == \"Random\"):\n",
        "            mask = mask_generator()\n",
        "\n",
        "        domainA = domainA.to(config.DEVICE)\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "        fake_B = gen_B(domainA, mask)\n",
        "        fake_A = gen_A(fake_B, mask)\n",
        "\n",
        "        save_image(mask, f\"Test/test_images_A/testmask_{idx}.png\")\n",
        "        save_image(domainA*0.5+0.5, f\"Test/test_images_A/testoriginal_{idx}.png\")\n",
        "        save_image(fake_B*0.5+0.5, f\"Test/test_images_A/testdomainB_{idx}.png\")\n",
        "        save_image(fake_A*0.5+0.5, f\"Test/test_images_A/testdomainA_{idx}.png\")\n",
        "\n",
        "        fake_A = gen_A(domainB, mask)\n",
        "        fake_B = gen_B(fake_A, mask)\n",
        "\n",
        "        save_image(mask, f\"Test/test_images_B/testmask_{idx}.png\")\n",
        "        save_image(domainB*0.5+0.5, f\"Test/test_images_B/testoriginal_{idx}.png\")\n",
        "        save_image(fake_B*0.5+0.5, f\"Test/test_images_B/testdomainB_{idx}.png\")\n",
        "        save_image(fake_A*0.5+0.5, f\"Test/test_images_B/testdomainA_{idx}.png\")\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    for idx, (domainB, domainA) in enumerate(loop):\n",
        "        domainA = domainA.to(config.DEVICE)\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "\n",
        "        save_image(domainA*0.5+0.5, f\"Test/Zebra100/original_{idx}.png\")\n",
        "        save_image(domainB*0.5+0.5, f\"Test/Horse100/original_{idx}.png\")\n",
        "\n",
        "        for mask_type in range(1,11):\n",
        "                mask = np.array(Image.open(f\"Test/Test_Masks/Mask_{mask_type}.png\").convert(\"RGB\"))\n",
        "                trans = T.Compose([T.ToTensor()])\n",
        "                mask = trans(mask)\n",
        "                mask = mask.to(config.DEVICE)\n",
        "\n",
        "                fake_B = gen_B(domainA, mask)\n",
        "                fake_A = gen_A(fake_B, mask)\n",
        "\n",
        "                save_image(fake_B*0.5+0.5, f\"Test/Zebra100/domainBMask{mask_type}_{idx}.png\")\n",
        "                save_image(fake_A*0.5+0.5, f\"Test/Zebra100/domainAMask{mask_type}_{idx}.png\")\n",
        "\n",
        "                fake_A = gen_A(domainB, mask)\n",
        "                fake_B = gen_B(fake_A, mask)\n",
        "\n",
        "                save_image(fake_B*0.5+0.5, f\"Test/Horse100/domainBMask{mask_type}_{idx}.png\")\n",
        "                save_image(fake_A*0.5+0.5, f\"Test/Horse100/domainAMask{mask_type}_{idx}.png\")\n",
        "\n",
        "\n",
        "\n",
        "###################### MAIN FUNCTION #######################\n",
        "def main():\n",
        "    #Initialize Discriminators and Generators\n",
        "    disc_A = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    disc_B = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    disc_AM = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    disc_BM = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    gen_A = Generator(img_channels=3).to(config.DEVICE)\n",
        "    gen_B = Generator(img_channels=3).to(config.DEVICE)\n",
        "\n",
        "    #Adam for Discriminators\n",
        "    opt_disc = optim.Adam(\n",
        "        list(disc_A.parameters()) + list(disc_B.parameters()) + list(disc_AM.parameters()) + list(disc_BM.parameters()),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    #Adam for Generators\n",
        "    opt_gen = optim.Adam(\n",
        "        list(gen_B.parameters()) + list(gen_A.parameters()),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    #Define L1 and Mean Squared Error loss\n",
        "    L1 = nn.L1Loss()\n",
        "    mse = nn.MSELoss()\n",
        "\n",
        "    #Load pretrained model\n",
        "    if config.LOAD_MODEL:\n",
        "        load_model(\n",
        "            config.CHECKPOINT_GEN_A, gen_A, opt_gen, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_GEN_B, gen_B, opt_gen, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_DISC_A, disc_A, opt_disc, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_DISC_B, disc_B, opt_disc, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_DISC_AM, disc_AM, opt_disc, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_DISC_BM, disc_BM, opt_disc, config.LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "\n",
        "    ############## CHOICE OF THE DATASET ###############\n",
        "    if(config.TRANSFORMATION == \"WinterToSummer\"):\n",
        "        dataset = Dataset(\n",
        "            domainA_dir=config.TRAIN_DIR+\"/trainWinter\", domainB_dir=config.TRAIN_DIR+\"/trainSummer\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            domainA_dir=config.TEST_DIR+\"/testWinter\", domainB_dir=config.TEST_DIR+\"/testSummer\", transform=config.transforms\n",
        "        )\n",
        "    elif(config.TRANSFORMATION == \"HorseToZebra\"):\n",
        "        dataset = Dataset(\n",
        "            domainA_dir=config.TRAIN_DIR+\"/trainHorse\", domainB_dir=config.TRAIN_DIR+\"/trainZebra\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            domainA_dir=config.TEST_DIR+\"/testHorse\", domainB_dir=config.TEST_DIR+\"/testZebra\", transform=config.transforms\n",
        "        )\n",
        "    elif(config.TRANSFORMATION == \"MonetToPhoto\"):\n",
        "        dataset = Dataset(\n",
        "            domainA_dir=config.TRAIN_DIR+\"/trainMonet\", domainB_dir=config.TRAIN_DIR+\"/trainPhotoMonet\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            domainA_dir=config.TEST_DIR+\"/testMonet\", domainB_dir=config.TEST_DIR+\"/testPhotoMonet\", transform=config.transforms\n",
        "        )\n",
        "    elif(config.TRANSFORMATION == \"AppleToOrange\"):\n",
        "        dataset = Dataset(\n",
        "            domainA_dir=config.TRAIN_DIR+\"/trainApple\", domainB_dir=config.TRAIN_DIR+\"/trainOrange\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            domainA_dir=config.TEST_DIR+\"/testApple\", domainB_dir=config.TEST_DIR+\"/testOrange\", transform=config.transforms\n",
        "        )\n",
        "\n",
        "\n",
        "    ############# DATALOADER #############\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True  #for faster training(non-paged cpu memory)\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    #Define the scalers\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    #Train the model\n",
        "    if(config.TRAIN_MODEL):\n",
        "\n",
        "        for epoch in range(config.NUM_EPOCHS):\n",
        "\n",
        "            #Set the models in training mode\n",
        "            disc_A.train()\n",
        "            disc_B.train()\n",
        "            disc_AM.train()\n",
        "            disc_BM.train()\n",
        "            gen_A.train()\n",
        "            gen_B.train()\n",
        "            train_fn(disc_A, disc_B, disc_AM, disc_BM, gen_B, gen_A, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler, config.LAMBDA_IDENTITY, config.LAMBDA_CYCLE, config.LAMBDA_MASK, config.LAMBDA_CYCLE_MASK, config.TRAIN_MASK)\n",
        "\n",
        "            #If SAVE_MODEL is set to True save the current model\n",
        "            if config.SAVE_MODEL:\n",
        "                save_model(gen_A, opt_gen, epoch ,filename=config.NEW_CHECKPOINT_GEN_A)\n",
        "                save_model(gen_B, opt_gen, epoch , filename=config.NEW_CHECKPOINT_GEN_B)\n",
        "                save_model(disc_A, opt_disc, epoch , filename=config.NEW_CHECKPOINT_DISC_A)\n",
        "                save_model(disc_B, opt_disc, epoch , filename=config.NEW_CHECKPOINT_DISC_B)\n",
        "                save_model(disc_AM, opt_disc, epoch , filename=config.NEW_CHECKPOINT_DISC_AM)\n",
        "                save_model(disc_BM, opt_disc, epoch , filename=config.NEW_CHECKPOINT_DISC_BM)\n",
        "\n",
        "    #Test the model\n",
        "    else:\n",
        "        #Set the models in evaluation mode\n",
        "        disc_A.eval()\n",
        "        disc_B.eval()\n",
        "        disc_AM.eval()\n",
        "        disc_BM.eval()\n",
        "        gen_A.eval()\n",
        "        gen_B.eval()\n",
        "\n",
        "        test_fn(gen_B, gen_A, test_loader, config.TEST_MASK)        #Start Test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}